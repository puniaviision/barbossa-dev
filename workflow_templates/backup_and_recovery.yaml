name: "Backup and Recovery Workflow"
description: "Comprehensive backup strategy for system data, databases, and configurations"
version: "1.0"

# Workflow configuration
trigger_type: "schedule"
schedule:
  cron: "0 1 * * *"  # Daily at 1 AM
  timezone: "UTC"

# Global variables
variables:
  backup_root: "/home/dappnode/backups"
  retention_days: 30
  compression_level: 6
  backup_date: "${system.date}"
  backup_timestamp: "${system.timestamp}"
  encryption_enabled: false
  remote_backup_enabled: false

# Notification settings
notifications:
  on_success:
    type: "log"
    message: "Backup workflow completed successfully"
  on_failure:
    type: "log"
    message: "Backup workflow failed"
    alert_level: "critical"

# Retry policy
retry_policy:
  max_retries: 2
  retry_delay: 300

# Workflow tasks
tasks:
  - id: "pre_backup_check"
    name: "Pre-Backup System Check"
    type: "health_check"
    checks:
      - type: "disk"
        threshold: 80  # Don't backup if disk is more than 80% full
      - type: "service"
        service_name: "docker"
    timeout: 60

  - id: "create_backup_directories"
    name: "Create Backup Directory Structure"
    type: "shell_command"
    dependencies: ["pre_backup_check"]
    command: |
      mkdir -p ${backup_root}/${backup_date}/{system,databases,configurations,projects,logs}
      chmod 700 ${backup_root}/${backup_date}
      echo "Backup directories created"
    timeout: 30

  - id: "backup_barbossa_system"
    name: "Backup Barbossa System Files"
    type: "shell_command"
    dependencies: ["create_backup_directories"]
    command: |
      cd /home/dappnode/barbossa-engineer
      
      # Backup main system files
      tar -czf ${backup_root}/${backup_date}/system/barbossa_system_${backup_timestamp}.tar.gz \
        --exclude='logs/*' \
        --exclude='backups/*' \
        --exclude='projects/*/node_modules' \
        --exclude='projects/*/.git' \
        --exclude='*.tmp' \
        --exclude='__pycache__' \
        *.py config/ security/ changelogs/ work_tracking/ workflow_templates/ web_portal/
      
      echo "Barbossa system backup completed"
    timeout: 600

  - id: "backup_configurations"
    name: "Backup System Configurations"
    type: "shell_command"
    dependencies: ["backup_barbossa_system"]
    command: |
      # Backup important system configurations
      tar -czf ${backup_root}/${backup_date}/configurations/system_configs_${backup_timestamp}.tar.gz \
        /etc/ssh/ \
        /etc/systemd/system/ \
        /etc/docker/ \
        /home/dappnode/.ssh/ \
        /home/dappnode/.bashrc \
        /home/dappnode/.profile \
        2>/dev/null || true
      
      # Backup crontab
      crontab -l > ${backup_root}/${backup_date}/configurations/crontab_${backup_timestamp}.txt 2>/dev/null || echo "No crontab" > ${backup_root}/${backup_date}/configurations/crontab_${backup_timestamp}.txt
      
      # Backup docker-compose files
      find /home/dappnode -name "docker-compose.yml" -o -name "docker-compose.yaml" | tar -czf ${backup_root}/${backup_date}/configurations/docker_compose_${backup_timestamp}.tar.gz -T - 2>/dev/null || true
      
      echo "Configuration backup completed"
    timeout: 300

  - id: "backup_databases"
    name: "Backup Databases"
    type: "shell_command"
    dependencies: ["backup_configurations"]
    command: |
      # Backup SQLite databases
      find /home/dappnode/barbossa-engineer -name "*.db" -exec cp {} ${backup_root}/${backup_date}/databases/ \; 2>/dev/null || true
      
      # Backup PostgreSQL databases (if running)
      if systemctl is-active postgresql > /dev/null 2>&1; then
        sudo -u postgres pg_dumpall > ${backup_root}/${backup_date}/databases/postgresql_${backup_timestamp}.sql 2>/dev/null || true
      fi
      
      # Backup Redis data (if running)
      if systemctl is-active redis > /dev/null 2>&1; then
        cp /var/lib/redis/dump.rdb ${backup_root}/${backup_date}/databases/redis_${backup_timestamp}.rdb 2>/dev/null || true
      fi
      
      echo "Database backup completed"
    timeout: 300

  - id: "backup_projects"
    name: "Backup Project Repositories"
    type: "shell_command"
    dependencies: ["backup_databases"]
    command: |
      if [ -d "/home/dappnode/barbossa-engineer/projects" ]; then
        cd /home/dappnode/barbossa-engineer/projects
        
        # Backup each project separately
        for project in */; do
          if [ -d "$project" ]; then
            echo "Backing up project: $project"
            tar -czf ${backup_root}/${backup_date}/projects/${project%/}_${backup_timestamp}.tar.gz \
              --exclude='node_modules' \
              --exclude='.git' \
              --exclude='*.tmp' \
              --exclude='dist' \
              --exclude='build' \
              "$project" 2>/dev/null || true
          fi
        done
      fi
      
      echo "Project backup completed"
    timeout: 900

  - id: "backup_important_logs"
    name: "Backup Important Log Files"
    type: "shell_command"
    dependencies: ["backup_projects"]
    command: |
      # Backup recent Barbossa logs
      find /home/dappnode/barbossa-engineer/logs -name "*.log" -mtime -7 | tar -czf ${backup_root}/${backup_date}/logs/barbossa_logs_${backup_timestamp}.tar.gz -T - 2>/dev/null || true
      
      # Backup system logs (last 7 days)
      find /var/log -name "*.log" -mtime -7 | head -50 | tar -czf ${backup_root}/${backup_date}/logs/system_logs_${backup_timestamp}.tar.gz -T - 2>/dev/null || true
      
      echo "Log backup completed"
    timeout: 300

  - id: "verify_backups"
    name: "Verify Backup Integrity"
    type: "shell_command"
    dependencies: ["backup_important_logs"]
    command: |
      cd ${backup_root}/${backup_date}
      
      # Verify all tar.gz files
      for backup_file in $(find . -name "*.tar.gz"); do
        if tar -tzf "$backup_file" > /dev/null 2>&1; then
          echo "✓ $backup_file: OK"
        else
          echo "✗ $backup_file: CORRUPTED"
          exit 1
        fi
      done
      
      echo "Backup verification completed"
    timeout: 300

  - id: "generate_backup_manifest"
    name: "Generate Backup Manifest"
    type: "python_script"
    dependencies: ["verify_backups"]
    script: |
      import os
      import json
      import hashlib
      import datetime
      from pathlib import Path
      
      backup_dir = Path("${backup_root}/${backup_date}")
      
      # Generate manifest
      manifest = {
          "backup_date": "${backup_date}",
          "backup_timestamp": "${backup_timestamp}",
          "created_at": datetime.datetime.now().isoformat(),
          "backup_type": "comprehensive",
          "retention_days": ${retention_days},
          "files": []
      }
      
      # Calculate checksums and file info
      for file_path in backup_dir.rglob("*"):
          if file_path.is_file():
              relative_path = file_path.relative_to(backup_dir)
              file_size = file_path.stat().st_size
              
              # Calculate MD5 checksum
              md5_hash = hashlib.md5()
              with open(file_path, 'rb') as f:
                  for chunk in iter(lambda: f.read(4096), b""):
                      md5_hash.update(chunk)
              
              file_info = {
                  "path": str(relative_path),
                  "size_bytes": file_size,
                  "size_human": f"{file_size / (1024*1024):.2f} MB" if file_size > 1024*1024 else f"{file_size / 1024:.2f} KB",
                  "md5_checksum": md5_hash.hexdigest(),
                  "type": "archive" if str(relative_path).endswith('.tar.gz') else "file"
              }
              manifest["files"].append(file_info)
      
      # Calculate total backup size
      total_size = sum(f["size_bytes"] for f in manifest["files"])
      manifest["total_size_bytes"] = total_size
      manifest["total_size_human"] = f"{total_size / (1024*1024*1024):.2f} GB" if total_size > 1024*1024*1024 else f"{total_size / (1024*1024):.2f} MB"
      manifest["file_count"] = len(manifest["files"])
      
      # Save manifest
      with open(backup_dir / "backup_manifest.json", 'w') as f:
          json.dump(manifest, f, indent=2)
      
      # Create human-readable summary
      summary = f"""
Backup Summary - ${backup_date}
===============================

Backup completed: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Total files: {manifest['file_count']}
Total size: {manifest['total_size_human']}
Location: {backup_dir}

Files included:
"""
      
      for file_info in manifest["files"]:
          summary += f"  - {file_info['path']} ({file_info['size_human']})\n"
      
      with open(backup_dir / "BACKUP_SUMMARY.txt", 'w') as f:
          f.write(summary)
      
      print(f"Backup manifest generated: {backup_dir}/backup_manifest.json")
      print(f"Total backup size: {manifest['total_size_human']}")
    timeout: 120

  - id: "cleanup_old_backups"
    name: "Clean Up Old Backups"
    type: "shell_command"
    dependencies: ["generate_backup_manifest"]
    command: |
      cd ${backup_root}
      
      # Remove backups older than retention period
      find . -maxdepth 1 -type d -name "20*" -mtime +${retention_days} -exec rm -rf {} \; 2>/dev/null || true
      
      # Clean up any orphaned files
      find . -maxdepth 1 -name "*.tar.gz" -mtime +${retention_days} -delete 2>/dev/null || true
      
      # Show remaining backups
      echo "Remaining backups:"
      ls -la | grep "^d" | grep "20"
      
      echo "Old backup cleanup completed"
    timeout: 300

  - id: "update_backup_index"
    name: "Update Backup Index"
    type: "python_script"
    dependencies: ["cleanup_old_backups"]
    script: |
      import json
      import datetime
      from pathlib import Path
      import glob
      
      backup_root = Path("${backup_root}")
      index_file = backup_root / "backup_index.json"
      
      # Load existing index or create new
      if index_file.exists():
          with open(index_file, 'r') as f:
              index = json.load(f)
      else:
          index = {"backups": [], "last_updated": None}
      
      # Find all backup directories
      backup_dirs = []
      for backup_dir in backup_root.glob("20*"):
          if backup_dir.is_dir():
              manifest_file = backup_dir / "backup_manifest.json"
              if manifest_file.exists():
                  with open(manifest_file, 'r') as f:
                      manifest = json.load(f)
                      backup_dirs.append({
                          "date": backup_dir.name,
                          "path": str(backup_dir),
                          "total_size": manifest.get("total_size_human", "Unknown"),
                          "file_count": manifest.get("file_count", 0),
                          "created_at": manifest.get("created_at", "Unknown")
                      })
      
      # Sort by date (newest first)
      backup_dirs.sort(key=lambda x: x["date"], reverse=True)
      
      # Update index
      index["backups"] = backup_dirs
      index["last_updated"] = datetime.datetime.now().isoformat()
      index["total_backups"] = len(backup_dirs)
      
      # Save updated index
      with open(index_file, 'w') as f:
          json.dump(index, f, indent=2)
      
      print(f"Backup index updated: {len(backup_dirs)} backups tracked")
    timeout: 60

# Optional remote backup tasks (conditional)
conditional_tasks:
  - id: "sync_to_remote"
    name: "Sync to Remote Storage"
    type: "shell_command"
    dependencies: ["update_backup_index"]
    condition: "${remote_backup_enabled}"
    command: |
      # Example: rsync to remote server
      # rsync -avz --delete ${backup_root}/${backup_date}/ user@remote-server:/backups/barbossa/
      echo "Remote backup sync would be performed here"
    timeout: 1800

# Recovery tasks (for manual recovery workflow)
recovery_tasks:
  - id: "list_available_backups"
    name: "List Available Backups"
    type: "shell_command"
    command: |
      cd ${backup_root}
      echo "Available backups:"
      ls -la | grep "^d" | grep "20" | awk '{print $9, $6, $7, $8}'
    timeout: 30

  - id: "restore_system_configs"
    name: "Restore System Configurations"
    type: "shell_command"
    command: |
      RESTORE_DATE="${restore_date:-$(ls ${backup_root} | grep '^20' | sort -r | head -1)}"
      
      if [ -f "${backup_root}/$RESTORE_DATE/configurations/system_configs_*.tar.gz" ]; then
        echo "Restoring system configurations from $RESTORE_DATE"
        cd /
        sudo tar -xzf ${backup_root}/$RESTORE_DATE/configurations/system_configs_*.tar.gz
        echo "System configurations restored"
      else
        echo "No system configuration backup found for $RESTORE_DATE"
        exit 1
      fi
    timeout: 300

  - id: "restore_barbossa_system"
    name: "Restore Barbossa System"
    type: "shell_command"
    command: |
      RESTORE_DATE="${restore_date:-$(ls ${backup_root} | grep '^20' | sort -r | head -1)}"
      
      if [ -f "${backup_root}/$RESTORE_DATE/system/barbossa_system_*.tar.gz" ]; then
        echo "Restoring Barbossa system from $RESTORE_DATE"
        cd /home/dappnode/barbossa-engineer
        tar -xzf ${backup_root}/$RESTORE_DATE/system/barbossa_system_*.tar.gz
        echo "Barbossa system restored"
      else
        echo "No Barbossa system backup found for $RESTORE_DATE"
        exit 1
      fi
    timeout: 300